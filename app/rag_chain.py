from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
import logging
import os
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

def create_qa_chain():
    """
    T·∫°o chu·ªói h·ªèi‚Äìƒë√°p (RetrievalQA) g·ªìm:
      - Load t√†i li·ªáu n·ªôi quy c√¥ng ty
      - Chia nh·ªè, t·∫°o embedding
      - L∆∞u v√†o Chroma VectorDB
      - Kh·ªüi t·∫°o m√¥ h√¨nh Ollama LLM
    """

    logger.info("üöÄ B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o RAG chain...")

    # 1Ô∏è‚É£ Load t√†i li·ªáu n·ªôi quy
    loader = TextLoader("data/company_rules.txt", encoding="utf-8")
    documents = loader.load()

    # 2Ô∏è‚É£ Chia nh·ªè vƒÉn b·∫£n
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)
    logger.info(f"‚úÖ S·ªë ƒëo·∫°n sau khi chia: {len(split_docs)}")

    # 3Ô∏è‚É£ T·∫°o embedding
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 4Ô∏è‚É£ T·∫°o / load VectorDB
    vectordb = Chroma.from_documents(
        split_docs, 
        embeddings, 
        persist_directory="./chroma_db"
    )
    vectordb.persist()

    # 5Ô∏è‚É£ C·∫•u h√¨nh m√¥ h√¨nh Ollama
    llm = OllamaLLM(
        model=os.getenv("OLLAMA_MODEL", "llama3"),
        temperature=float(os.getenv("LLM_TEMPERATURE", "0.2")),
        top_k=int(os.getenv("LLM_TOP_K", "40")),
        repeat_penalty=float(os.getenv("LLM_REPEAT_PENALTY", "1.2")),
        num_ctx=int(os.getenv("LLM_NUM_CTX", "4096"))
    )

    # 6Ô∏è‚É£ Prompt m·∫´u
    template = """
    B·∫°n l√† tr·ª£ l√Ω AI hi·ªÉu r√µ n·ªôi quy c√¥ng ty. D·ª±a tr√™n th√¥ng tin d∆∞·ªõi ƒë√¢y:
    {context}
    H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c cho c√¢u h·ªèi sau: {question}
    """
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])

    # 7Ô∏è‚É£ T·∫°o chu·ªói h·ªèi‚Äìƒë√°p
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": prompt}
    )

    logger.info("üéØ QA chain ƒë√£ s·∫µn s√†ng")
    return qa_chain
